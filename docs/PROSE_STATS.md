# Prose Statistics Algorithms and Legend

This document describes how the Prose Minion VS Code extension computes each prose statistic, what it means, and any caveats. Where applicable, the implementation references `src/tools/measure/passageProseStats/index.ts`.

## Tokenization Basics

- Words: lowercased tokens split on whitespace, with all non-letters removed except `'` (apostrophes) for intermediate processing, then trimmed. Implementation: `tokenizeWords()`.
- Sentences: naÃ¯vely split on `.`, `!`, or `?` (one or more). Regex: `/[.!?]+/`.
- Paragraphs: split on a blank line (double newline). Regex: `/\n\s*\n/`.

These choices favor performance and determinism over linguistic completeness.

## Metrics (How Theyâ€™re Calculated)

- Word Count
  - Count of tokens split by whitespace. Regex: `/\s+/`.
  - Function: `countWords()`.

- Sentence Count
  - Count of substrings between sentence-ending punctuation `.!?`.
  - Function: `countSentences()`.

- Paragraph Count
  - Count of blocks separated by a blank line.
  - Function: `countParagraphs()`.

- Average Words per Sentence (Avg W/S)
  - `wordCount / sentenceCount`. Guard against divide-by-zero.

- Average Sentences per Paragraph
  - `sentenceCount / paragraphCount`. Guard against divide-by-zero.

- Dialogue Percentage
  - Percentage of tokens appearing inside double quotes.
  - Steps: find all `"â€¦"` spans, concatenate, re-count words, divide by total words.
  - Function: `calculateDialoguePercentage()`.
  - Caveat: Only double quotes are considered; locale-specific quotes and nested quotes are not handled.

- Lexical Density (%)
  - Percentage of content words (non-stopwords) over total tokens.
  - Steps: tokenize words â†’ count tokens not in a compact English stopword set â†’ `(content / total) Ã— 100`.
  - Function: `calculateLexicalDensityPercent()`.
  - Note: This is different from Typeâ€“Token Ratio (TTR). Lexical density approximates information load vs function words.

- Stopword Ratio (%)
  - Percentage of tokens that are stopwords.
  - Steps: tokenize â†’ count tokens that are stopwords â†’ `(stop / total) Ã— 100`.
  - Function: `calculateStopwordRatioPercent()`.

- Unique Word Count
  - Count of distinct word forms (set size after tokenization).
  - Function: `countUniqueWords()`.

- Hapax % and Hapax Count
  - Hapax legomena are words that occur exactly once.
  - Steps: build frequency map; `hapaxCount = count(freq == 1)`; `hapax% = (hapaxCount / total tokens) Ã— 100`.
  - Functions: `calculateHapaxPercent()`, `calculateHapaxCount()`.

- Typeâ€“Token Ratio (TTR) %
  - `(unique / total) Ã— 100`.
  - Function: `calculateTypeTokenRatioPercent()`.

- Reading Time
  - Minutes: `wordCount / 240` (default WPM = 240). Hours = `minutes / 60`.
  - Short label: rounded minutes (e.g., `"5m"`).
  - Functions: `estimateReadingTimeMinutes()` and display logic in `analyze()`.

- Readability Score (Simplified Flesch Reading Ease)
  - Approximation without syllables: `100 - (avgWordsPerSentence Ã— 2)`.
  - Range: 0â€“100; higher means easier to read.
  - Function: `calculateReadabilityScore()`.

- Readability Grade (FKGL)
  - Formula: `0.39 Ã— (words/sentences) + 11.8 Ã— (syllables/words) â€“ 15.59`.
  - A lightweight syllable estimator counts vowel groups with adjustments (silent `e`, `le` ending, etc.).
  - Functions: `estimateSyllables()`, `calculateFKGLGrade()`.

- Pacing (Qualitative)
  - Based on Avg Words per Sentence:
    - `< 10`: Fast (short sentences)
    - `< 20`: Moderate
    - `< 30`: Slow (longer sentences)
    - `>= 30`: Very slow (very long sentences)
  - Function: `determinePacing()`.

- Word Length Distribution (%)
  - Buckets by token length after removing apostrophes: `1â€“3`, `4â€“6`, `7+` letters.
  - Percentages over total tokens.
  - Function: `calculateWordLengthDistribution()`.

- Word Frequency (Enhanced)
  - Top Words: count of most frequent content words (stopwords excluded by default); size configurable (default 100).
  - Stopwords: table of top stopwords and total stopword token count (complements Stopword Ratio).
  - Hapax: hapax count, percent, and an alphabetized list (display capped in UI; full count computed).
  - POS: parts-of-speech lists (nouns, verbs, adjectives, adverbs) via wink-pos-tagger; if unavailable, sections show a short note.
  - Nâ€‘grams: Top bigrams and trigrams (each top 20 by default).
  - Word Length Histogram: slider-style bars 1â€“10 characters with proportional blocks and percentages.
  - Lemmas (optional): groups common inflections into lemma buckets for an alternate â€œTop Lemmasâ€ view.

## Chapter Metrics (Multi-file Modes)

When measuring â€œManuscriptsâ€ or â€œChaptersâ€, each matched file is treated as one chapter.

- Chapter Count: number of files included.
- Average Chapter Length: mean words per file (rounded).
- Per-chapter stats: the same metrics as above are computed per file and rendered in:
  - A summary â€œChapterâ€‘byâ€‘Chapter Prose Statisticsâ€ table.
  - Optional â€œChapter Detailsâ€ section (one pivoted markdown table per chapter) included on Copy/Save if you confirm.

## Legend (UI Labels â†’ Meaning)

- ğŸ“ Word Count â†’ total tokens
- ğŸ“ Sentence Count â†’ count of `.!?` delimited sentences
- ğŸ“‘ Paragraph Count â†’ count of blank-line separated blocks
- âš–ï¸ Avg Words per Sentence â†’ mean words per sentence
- ğŸ“ Avg Sentences per Paragraph â†’ mean sentences per paragraph
- â±ï¸ Reading Time â†’ minutes at 240 wpm (short label)
- ğŸ¯ Pacing â†’ qualitative based on sentence length
- ğŸ’¬ Dialogue Percentage â†’ % of tokens inside `"â€¦"`
- ğŸ¨ Lexical Density â†’ % of non-stopwords over total tokens
- ğŸ§¹ Stopword Ratio â†’ % of tokens in stopword list
- ğŸŒ± Hapax % / ğŸŒ± Hapax Count â†’ % and count of words occurring exactly once
- ğŸ”€ Type-Token Ratio â†’ unique/total tokens Ã— 100
- ğŸ“– Readability Score â†’ simplified Flesch Reading Ease (0â€“100)
- ğŸ“ Readability Grade (FKGL) â†’ Fleschâ€“Kincaid grade estimate
- ğŸ” Unique Words â†’ distinct word forms
- â³ Reading Time (min) â†’ numeric minutes

## Rounding and Presentation

- Percentages: typically shown with 1 decimal place (e.g., `52.6%`).
- Averages: shown with 1 decimal place.
- Large counts: localized with thousands separators.
- Readability score and FKGL: shown with 1 decimal place.

## Known Limitations

- Tokenization and sentence splitting are heuristic and language-agnostic.
- Dialogue detection considers only double quotes.
- Stopword list is compact; domain-specific terms may affect density.
- Readability Score is a simplified proxy; FKGL uses a heuristic syllable estimator.

## Publishing Standards Interop

When a publishing standards preset is selected, comparisons use the same units:

- Lexical density: percent 0â€“100 (content-word ratio).
- Dialogue percentage: percent 0â€“100.
- Word length distribution: percent 0â€“100 for each bucket.
- Average words/sentence, sentences/paragraph, unique words, etc. compare to numeric ranges from the standards dataset.
